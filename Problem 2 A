import torch.nn as nn

class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.downsample = None
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)
        return out

class ResNet10(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.in_channels = 16

        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.relu = nn.ReLU(inplace=True)

        self.layer1 = self._make_layer(16, blocks=3, stride=1)
        self.layer2 = self._make_layer(32, blocks=3, stride=2)
        self.layer3 = self._make_layer(64, blocks=4, stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, num_classes)

    def _make_layer(self, out_channels, blocks, stride):
        layers = []
        layers.append(BasicBlock(self.in_channels, out_channels, stride))
        self.in_channels = out_channels
        for _ in range(1, blocks):
            layers.append(BasicBlock(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x
import torch.optim as optim

model_2a = ResNet10(num_classes=10).to(device)
criterion_2a = nn.CrossEntropyLoss()
optimizer_2a = optim.Adam(model_2a.parameters(), lr=0.001)

EPOCHS_2A = 40
train_losses_2a = []
test_accuracies_2a = []
import time
import torch

start_time_2a = time.time()

for epoch in range(EPOCHS_2A):
    model_2a.train()
    loss_sum = 0

    for x, y in trainloader:
        x, y = x.to(device), y.to(device)
        optimizer_2a.zero_grad()
        out = model_2a(x)
        loss = criterion_2a(out, y)
        loss.backward()
        optimizer_2a.step()
        loss_sum += loss.item()

    train_losses_2a.append(loss_sum / len(trainloader))

    model_2a.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for x, y in testloader:
            x, y = x.to(device), y.to(device)
            out = model_2a(x)
            _, pred = torch.max(out, 1)
            total += y.size(0)
            correct += (pred == y).sum().item()

    acc = 100 * correct / total
    test_accuracies_2a.append(acc)

    print(f"Epoch [{epoch+1}/{EPOCHS_2A}] Loss: {train_losses_2a[-1]:.4f} | Accuracy: {acc:.2f}%")

end_time_2a = time.time()
print("Total training time 2(a):", end_time_2a - start_time_2a)
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(train_losses_2a)
plt.title("Training Loss 2(a)")

plt.subplot(1,2,2)
plt.plot(test_accuracies_2a)
plt.title("Evaluation Accuracy 2(a)")

plt.show()
